[
["index.html", "MSc Conversion in Psychological Studies/Science Overview 0.1 Intended Learning Outcomes", " MSc Conversion in Psychological Studies/Science Emily Nordmann 2019-06-23 Overview This book contains the materials for students on the MSc Conversion in Psychological Studies/Science. This course is a one-year postgraduate degree where students who already hold a non-psychology undergraduate degree receive a British Psychological Society accredited MSc. The students are typically a diverse cohort and range from those with no STEM or programming background to engineering and computing science graduates. Compared to the undergraduate degree, the students are older, and there is a greater incidence of computer anxiety. As a consequence of the intense nature of the programme and the diversity of the cohort, the approach to R is slightly different to that taken in the undergraduate programmes. The focus for the MSc is to provide a basic but solid competency in core data skills and statistics that can be built on in further study. Students who wish to push themselves beyond the core competencies are encouraged to consult the MSc Data Skills course where they can learn about e.g., simulation and custom functions. To support those students who may have very limited computer literacy, the beginning stages are more supported than in the undergraduate programme e.g., with an increased use of screenshots and explanations for terminiology. In this course you will learn core data skills that allow you to manipulate and analyse quantitative data, a key component of an accredited psychology programme. Each week we will build your skills through pre-class, in-class, and homework activities. In addition to this book, there are video walkthroughs of each in-class activity available on Moodle and there will be drop-in help sessions run by our Graduate Teaching Assistants. The ability to work with quantitative data is a key skill for psychologists and by using R as our tool we can also promote reproducible research practices. Although it may seem like writing a programming script is more time-consuming than other point-and-click software you may have used, this is not the case! Once you have a script you can easily re-run your analysis without having to go through each step again manually which is a) easier and b) less likely to result in errors if you do something slightly different or forget one of the steps. Crucially, with an analysis script other researchers can also see how you got from the raw data to the statistics you report in your final paper. Sharing analysis scripts online on sites such as the Open Science Framework is now seen as an important open science practice. Even if you don’t continue with quantitative research in the future, the skills you develop on this course will allow you to evaluate quantitative research and to understand what goes on behind the scenes with data before the conclusions are presented. 0.1 Intended Learning Outcomes By the end of this course students will be able to: Clean and wrangle data into appropriate forms for analysis Visualise data using a range of plots Conduct and interpret a core set of statistical tests (chi-square, t-test, correlation, ANOVA, regression) "],
["programming-basics.html", "Chapter 1 Programming Basics 1.1 R and RStudio 1.2 Functions and arguments 1.3 Base R and packages 1.4 Objects 1.5 Help and additional resources 1.6 Debugging tips 1.7 Test yourself", " Chapter 1 Programming Basics This chapter is intended to introduce key programming terminology to those students who may have had very little experience with coding, or indeed with computers. As a result of the diverse cohort, even basic practical steps are broken down and supported by screenshots. In my experience, for those with programming anxiety it is very difficult to overcome a bad start to learning R therefore extra effort is paid towards softening the learning curve in these initial labs and providing activities that have a tangible output so that the students can visualise their success. In this chapter we will go over some basic programming concepts and terminology, common pitfalls, helpful hints, and where to get help. Those of you who have no programming experience should find this chapter particularly helpful, however, even if you’ve used R before there may be some helpful hints and tips so please make sure you read through this chapter before Lab 1. We don’t expect you to memorise the information that is contained in this chapter and some sections of it will make more sense when you start writing your own code in Lab 1 - just make sure you know what help is available! 1.1 R and RStudio For this course, you need two different bits of software, R and RStudio. R is a programming language that you will write code in and RStudio is an Integrated Development Environment (IDE) which makes working with R easier. Think of it as knowing English and using a plain text editor like NotePad to write a book versus using a word processor like Microsoft Word. You could do it, but it wouldn’t look as good and it would be much harder without things like spell-checking and formatting. In a similar way, you can use R without RStudio but we wouldn’t reccommend it. The key thing to remember is that although you will do all of your work using RStudio for this course, you are actually using two pieces of software which means that from time-to-time, both of them may have separate updates. All of the University of Glasgow computers should already have R and RStudio installed, however, both are freely available so you may wish to install them on your own machine. 1.1.1 Getting to know RStudio RStudio has a console that you can try out code in (appearing as the bottom left window in Figure 1.1), there is a script editor (top left), a window showing functions and objects you have created in the “Environment” tab (top right window in the figure), and a window that shows plots, files packages, and help documentation (bottom right). Figure 1.1: RStudio interface You will learn more about how to use the features included in RStudio throughout this course, however, we highly reccommend watching RStudio Essentials 1 from the RStudio team. The video lasts ~30 minutes and gives a tour of the main parts of RStudio. 1.2 Functions and arguments Functions in R execute specific tasks and normally take a number of arguments (if you’re into linguistics you might want to think as these as verbs that require a subject and an object). You can look up all the arguments that a function takes by using the help documentation by using the format ?function. Some arguments are required, and some are optional. Optional arguments will often use a default (normally specified in the help documentation) if you do not enter any value. As an example, let’s look at the help documentation for the function rnorm() which randomly generates a set of numbers with a normal distribution. Open up RStudio and in the console, type the following code: ?rnorm The help documentation for rnorm() should appear in the bottom right help panel. In the usage section, we see that rnorm() takes the following form: rnorm(n, mean = 0, sd = 1) In the arguments section, there are explanations for each of the arguments. n is the number of observations we want to create, mean is the mean of the data points we will create and sd is the standard deviation of the set. In the details section it notes that if no values are entered for mean and sd it will use a default of 0 and 1 for these values. Because there is no default value for n it must be specified otherwise the code won’t run. Let’s try an example and just change the required argument n to ask R to produce 5 random numbers. Copy and paste the following code into the console. set.seed(12042016) rnorm(n = 5) ## [1] -0.2896163 -0.6428964 0.5829221 -0.3286728 -0.5110101 These numbers have a mean of 0 and an SD of 1. Now we can change the additional arguments to produce a different set of numbers. rnorm(n = 5, mean = 10, sd = 2) ## [1] 13.320853 9.377956 10.235461 9.811793 13.019102 This time R has still produced 5 random numbers, but now this set of numbers has a mean of 10 and an sd of 2 as specified. Always remember to use the help documentation to help you understand what arguments a function requires. If you’re looking up examples of code online, you may often see code that starts with the function set.seed(). This function controls the random number generator - if you’re using any functions that generate numbers randomly (such as rnorm()), running set.seed() will ensure that you get the same result (in some cases this may not be what you want to do). We call set.seed() in this example because it means that you will get the same random numbers as this book. 1.2.1 Argument names In the above examples, we have written out the argument names in our code (e.g., n, mean, sd), however, this is not strictly necessary. The following two lines of code will both produce the same result: rnorm(n = 6, mean = 3, sd = 1) rnorm(6, 3, 1) Importantly, if you do not write out the argument names, R will use the default order of arguments, that is for rnorm it will assume that the first number you enter is n. the second number is mean and the third number is sd. If you write out the argument names then you can write the arguments in whatever order you like: rnorm(sd = 1, n = 6, mean = 3) When you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see examples of code online that do not use argument names so it is important to be able to understand which argument each bit of code is referring to (or look up the help documentation to check). In this course, we will always write out the argument names the first time we use each function, however, in subsequent uses they may be omitted. 1.2.2 Tab auto-complete One very useful feature of RStudio is the tab auto-complete for functions (see Figure 1.2. If you write the name of the function and then press the tab key, RStudio will show you the arguments that function takes along with a brief description. If you press enter on the argument name it will fill in the name for you, just like auto-complete on your phone. This is incredibly useful when you are first learning R and you should remember to use this feature frequently. Figure 1.2: Tab auto-complete 1.3 Base R and packages When you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as Base R and there is a useful cheat sheet that shows many Base R functions here. However, the power of R is that it is extendable and open source - put simply, if a function doesn’t exist or doesn’t work very well, anyone can create a new package that contains data and code to allow you to perform new tasks. You may find it useful to think of Base R as the default apps that come on your phone and packages as additional apps that you need to download separately. 1.3.1 Installing and loading packages In order to use a package, you must first install it. The following code installs the package tidyverse, a package we will use very frequently in this course. install.packages(&quot;tidyverse&quot;) You only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it. To load packages we use the function library(). Typically you would start any analysis script by loading all of the packages you need, but we will come back to that in Lab 1. library(tidyverse) Now that we’ve loaded the tidyverse package we can use any of the functions it contains but remember, you need to run the library() function every time you start R. All of the University of Glasgow computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine. Please do not install any new packages on the university machines. 1.3.2 Package updates In addition to updates to R and RStudio, the creators of packages also sometimes update their code. This can be to add functions to a package, or it can be to fix errors. One thing to avoid is unintentionally updating an installed package. When you run install.packages() it will always install the latest version of the package and it will overwrite any older versions you may have installed. Sometimes this isn’t a problem, however, sometimes you will find that the update means your code no longer works as the package has changed substantially. It is possible to revert back to an older version of a package but try to avoid this anyway. To avoid accidentally overwriting a package with a later version, you should never include install.packages() in your analysis scripts in case you, or someone else runs the code by mistake. Remember, the University of Glasgow computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine. 1.3.3 Package conflicts There are thousands of different R packages with even more functions. Unfortunately, sometimes different packages have the same function names. For example, the packages dplyr and MASS both have a function named select(). If you load both of these packages, R will produce a warning telling you that there is a conflict. library(dplyr) library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select In this case, R is telling you that the function select() in the dplyr is being hidden (or ‘masked’) by another function with the same name. If you were to try and use select(), R would use the function from the package that was loaded most recently - in this case it would use the function from MASS. If you want to specify which package you want to use for a particular function you can use code in the format package::function, for example: dplyr::select(...) MASS::select(...) 1.4 Objects A large part of your coding will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses.You assign content to an object using &lt;-. Copy and paste the following code into the console and run it. You should see that name, age, today, new_year, and data appear in the environment pane. name &lt;- &quot;emily&quot; age &lt;- 15 + 18 today &lt;-Sys.Date() new_year &lt;- as.Date(&quot;2020-01-01&quot;) data &lt;- rnorm(n = 10, mean = 15, sd = 3) Figure 1.3: Objects in the environment Note that in these examples, name,age, and new_year would always contain the values emily, 33, and the date of New Year’s Day 2020, however, today will draw the date from the operating system and data will be a randomly generated set of data so the values of these objects will not be static. Importantly, objects can be involved in calculations and can interact with each other. For example: age + 10 new_year - today mean(data) ## [1] 43 ## Time difference of 192 days ## [1] 17.66644 Finally, you can store the result of these operations in a new object: decade &lt;- age + 10 You may find it helpful to read &lt;- as contains, e.g., name contains the text emily. You will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along, however, for now it is enough to understand that they are a way of saving values, that these values can numbers, text, or the result of operations, and that they can be used in further operations to create new variables. You may also see objects referred to as ‘variables’. There is a difference between the two in programming terms, however, they are used synonomously very frequently. 1.5 Help and additional resources Figure 1.4: The truth about programming Getting good at programming really means getting good trying stuff out, searching for help online, and finding examples of code to copy. If you are having difficulty with any of the exercises contained in this book then you can ask for help on Slack or Moodle, however, learning to problem-solve effectively is a key skill that you need to develop throughout this course. Use the help documentation. If you’re struggling to understand how a function works, remember the ?function command. If you get an error message, copy and paste it in to Google - it’s very likely someone else has had the same problem. In addition to these course materials there are a number of excellent resources for learning R: R Cookbook StackOverlow R for Data Science Search or use the #rstats hashtag on Twitter If you would like to push yourself further with R, we reccomend working through R for Data Science in full and/or the materials for the MSc Data Skills course run by Prof. Lisa DeBruine and Dr. Dale Barr at the University of Glasgow. 1.6 Debugging tips A large part of coding is trying to figure why your code doesn’t work and this is true whether you are a novice or an expert. As you progress through this course you should keep a record of mistakes you make and how you fixed them. In each chapter we will provide a number of common mistakes to look out for but you will undoubtedly make (and fix!) new mistakes yourself. Have you loaded the correct packages for the functions you are trying to use? One very common mistake is to write the code to load the package, e.g., library(tidyverse) but then forget to run it. Have you made a typo? Remember data is not the same as DATA and t.test is not the same as t_test. Is there a package conflict? Have you tried specifying the package and function with package::function? Is it definitely an error? Not all red text in R means an error - sometimes it is just giving you a message with information. 1.7 Test yourself Question 1. Why should you never include the code install.packages() in your analysis scripts? You should use library() instead Packages are already part of Base R You (or someone else) may accidentally install a package update that stops your code working You already have the latest version of the package Explain This Answer Remember, when you run install.packages() it will always install the latest version of the package and it will overwrite any older versions of the package you may have installed. Question 2.What will the following code produce? rnorm(6, 50, 10) A dataset with 10 numbers that has a mean of 6 and an SD of 50 A dataset with 6 numbers that has a mean of 50 and an SD of 10 A dataset with 50 numbers that has a mean of 10 and an SD of 6 A dataset with 50 numbers that has a mean of 10 and an SD of 6 Explain This Answer The default form for rnorm() is rnorm(n, mean, sd). If you need help remembering what each argument of a function does, look up the help documentation by running ?rnorm Question 3. If you have two packages that have functions with the same name and you want to specify exactly which package to use, what code would you use? package::function function::package library(package) install.packages(package) Explain This Answer You should use the form package::function, for example dplyr::select. Remember that when you first load your packages R will warn you if any functions have the same name - remember to look out for this! "],
["rm1-lab-1.html", "Chapter 2 RM1: Lab 1 2.1 Pre-class activities 2.2 In-class activities 2.3 Debugging tips 2.4 Test yourself", " Chapter 2 RM1: Lab 1 This is the first lab in which students will be introduced to R. The key aims of this lab are a) to introduce R as a tool that will support their open science practices such as scripting and replicability and b) to reduce any anxiety about using R by starting out with small, stepped-out activities that produce a tangible output where success is visible. 2.1 Pre-class activities There are nine activities in total for this pre-lab, but don’t worry, they are broken down into very small steps! 2.1.1 Activity 1: Programming basics Please read Programming Basics and watch the two R videos available on Moodle. 2.1.2 Activity 2: Create the working directory If you want to load data into R, or save the ouput of what you’ve created (which you almost always will want to do), you first need to tell R where the working directory is. All this means is that we tell R where the files we need (such as raw data) are located and where we want to save any files you have created. Think of it just like when you have different subjects, and you have seperate folders for each topic e.g. biology, history and so on. When working with R, it’s useful to have all the data sets and files you need in one folder. We reccommend making a new folder called “Research Methods R Labs” with sub-folders for each lab and saving any data, scripts, and portfolio files for each lab in these folders. We suggest that you create this folder on the M: drive. This is your personal area on the University network that is safe and secure so it is much better than flashdrives or desktops. Figure 2.1: Lab folder structure First, choose a location for your lab work and then create the necessary folders for the first three R labs for RM1. 2.1.3 Activity 3: Set the working directory Once you have created your folders, open R Studio. To set the working directory click Session -&gt; Set Working Directory -&gt; Choose Directory and then select the RM1 Lab 1 folder as your working directory. Figure 2.2: Setting the working directory 2.1.4 R Markdown for R lab work and portfolio assignments For the R lab work and portfolio assignments you will use a worksheet format called R Markdown (abbreviated as Rmd) which is a great way to create dynamic documents with embedded chunks of code. These documents are self-contained and fully reproducible (if you have the necessary data, you should be able to run someone else’s analyses with the click of a button) which makes it very easy to share. This is an important part of your open science training as one of the reasons we are using RStudio is that it enables us to share open and reproducible information. Using these worksheets enables you to keep a record of all the code you write during the labs, and when it comes time for the portfolio assignments, we can give you a task you can and then fill in the required code. For more information about R Markdown feel free to have a look at their main webpage sometime http://rmarkdown.rstudio.com. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then knit it using the package knitr to create your document as either a webpage (HTML), a PDF, or Word document (.docx). 2.1.5 Activity 4: Open and save a new R Markdown document To open a new R Markdown document click the ‘new item’ icon and then click ‘R Markdown’. You will be prompted to give it a title, call it “RM1 Lab 1”. Also, change the author name to your GUID as this will be good practice for the portfolio assignments. Keep the output format as HTML. Figure 2.3: Opening a new R Markdown document Once you’ve opened a new document be sure to save it by clicking File -&gt; Save as. Name this file “Pre-lab 1”. If you’ve set the working directory correctly, you should now see this file appear in your file viewer pane. Figure 2.4: New file in working directory 2.1.6 Activity 5: Create a new code chunk When you first open a new R Markdown document you will see a bunch of welcome text that looks like this: Figure 2.5: New R Markdown text Do the following steps: * Delete everything below line 7 * On line 8 type “About me” * Click Insert -&gt; R Your Markdown document should now look something like this: Figure 2.6: New R chunk What you have created is a code chunk. In R Markdown, anything written in the white space is regarded as normal text, and anything written in a grey code chunk is assumed to be code. This makes it easy to combine both text and code in one document. When you create a new code chunk you should notice that the grey box starts and ends with three back ticks ```. One common mistake is to accidentally delete these back ticks. Remember, code chunks are grey and text entry is white - if the colour of certain parts of your Markdown doesn’t look right, check that you haven’t deleted the back ticks. 2.1.7 Activity 6: Write some code Now we’re going to use the code examples you read about in Programming Basics to add some simple code to our R Markdown dcoument. In your code chunk write the below code but replace the values of name/age/birthday with your own details). Note that text values and dates need to be contained in quotation marks but numerical values do not. Missing and/or unnecessary quotation marks are a common cause of code not working - remember this! name &lt;- &quot;Emily&quot; age &lt;- 33 today &lt;- Sys.Date() next_birthday &lt;- as.Date(&quot;2019-07-11&quot;) 2.1.8 Running code When you’re working in an R Markdown document, there are several ways to run your lines of code. First, you can highlight the code you want to run and then click Run -&gt; Run Selected Line(s), however this is very slow. Figure 2.7: Slow method of running code Alternatively, you can press the green “play” button at the top-right of the code chunk and this will run all lines of code in that chunk. Figure 2.8: Slightly better method of running code Even better though is to learn some of the keyboard shortcuts for RStudio. To run a single line of code, make sure that the cursor is in the line of code you want to run (it can be anywhere) and press ctrl + enter. If you want to run all of the code in the code chunk, press ctrl + shift + enter. Learn these shortcuts, they will make your life easier! 2.1.9 Activity 7: Run your code Run your code using one of the methods above. You should see the variables name, age, today, and next_birthday appear in the environment pane. 2.1.10 Activity 8: Inline code An incredibly useful feature of R Markdown is that R can insert values into your writing using inline code. If you’ve ever had to copy and paste a value or text from one file in to another, you’ll know how easy it can be to make mistakes. Inline code avoids this. It’s easier to show you what inline code does rather than to explain it so let’s have a go. First, copy and paste this text exactly (do not change anything) to the white space underneath your code chunk. My name is `r name` and I am `r age` years old. It is `r next_birthday - today` days until my birthday. 2.1.11 Activity 9: Knitting your file Nearly finished! As our final step we are going to “knit” our file. This simply means that we’re going to compile our code into a document that is more presentable. To do this click Knit -&gt; Knit to HMTL. R Markdown will create a new HTML document and it will automatically save this file in your working directory. As if by magic, that slightly odd bit of text you copied and pasted now appears as a normal sentence with the values pulled in from the objects you created. My name is Emily and I am 33 years old. It is 18 days until my birthday. We’re not going to use this function very often in the rest of the course but hopefully you can see just how useful this would be when writing up a report with lots of numbers! R Markdown is an incredibly powerful and flexible format - this book was written using it! If you want to push yourself with R, additional functions and features of R Markdown would be a good place to start. Before we finish, there are a few final things to note about knitting that will be useful for the portfolio and mini-project: R Markdown will only knit if your code works - this is a good way of checking for the portfolio assignments whether you’ve written legal code! You can choose to knit to a Word document rather than HTML. This can be useful for e.g., sharing with others, however, it may lose some functionality and it probably won’t look as good so we’d reccommend always knitting to HTML. You can choose to knit to PDF, however, this requires an LaTex installation and is quite complicated. If you don’t already know what LaTex is and how to use it, do not knit to PDF. If you do know how to use LaTex, you don’t need us to give you instructions! R will automatically open the knitted HTML file in the viewer, however, you can also navigate to the folder it is stored in and open the HTML file in your web browser (e.g., Chrome or Firefox). 2.1.12 Finished And you’re done! On your very first time using R you’ve not only written functioning code but you’ve written a reproducible output! You could send someone else your R Markdown document and they would be able to produce exactly the same HTML document as you, just by pressing knit. The key thing we want you to take away from this pre-lab is that R isn’t scary. It might be very new to a lot of you, but we’re going to take you through it step-by-step. You’ll be amazed at how quickly you can start producing professional-looking data visualisations and analysis. If you have any questions about anything contained in this pre-lab or in Programming Basics, you can use the Slack forum or ask your lab tutor. 2.2 In-class activities Part of becoming a psychologist is asking questions and gathering data to enable you to answer these questions effectively. It is very important that you understand all aspects of the research process such as experimental design, ethics, data management and visualisation. In this class, you will continue to develop reproducible scripts. This means scripts that completely and transparently perform an analysis from start to finish in a way that yields the same result for different people using the same software on different computers. And transparency is a key value of science, as embodied in the “trust but verify” motto. When you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. The topic of open science is a big debate in the scientific community at the moment. Some classic psychological experiments have been found not to be replicable and part of the explanation for this has been a historical lack of transparency about data and analysis methods. If you’d like more information on this, you may find the following articles interesting: Study delivers bleak verdict on validity of psychology experiment results Low replicability in psychological science As part of your skill development, it is important that you work with data so that you can become confident and competent in your management and analysis of data. In the labs, we will work with real data that has been shared by other researchers. 2.2.1 Getting data ready to work with Today in the lab you will learn how to load the packages required to work with our data. You’ll then load the data into RStudio before getting it organised into a sensible format that relates to our research question. If you can’t remember what packages are, go back and revise Programming Basics. 2.2.2 Activity 1: Set-up Before we begin working with the data we need to do some set-up. If you need help with any of these steps, you should refer to the pre-lab activities: * Download the data files from Moodle into your Lab 1 folder. * Set the working directory to your Lab 1 folder. * Open a new R Markdown document and save it in your working directory. Call the file “In-class 1”. * Delete the default R Markdown welcome text and insert a new code chunk. * You can use the white space to take any notes that might help you for each activity. 2.2.3 Activity 2: Load in the package Today we need to use the tidyverse package. You will use this package in every single lab on this course as the functions it contains are those we use for data wrangling, descriptive statistics, and visualisation. To load the tidyverse type the following code into your code chunk and then run it. library(tidyverse) 2.2.4 Open data For this lab we are going to be using real data from the following paper: Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from, ‘Web-based Positive Psychology Interventions: A Reexamination of Effectiveness’. Journal of Open Psychology Data, 6(1). We reccomend that you read through this paper and open up the .csv files in order to understand the data better but briefly, the files contains data from two scales, the Authentic Happiness Inventory (AHI) and the Center for Epidemiological Studies Depression (CES-D) scale, as well as demographic information about participants. 2.2.5 Activity 3: Read in data Now we can read in the data. To do this we will use the function read_csv() that allows us to read in .csv files. There are also functions that allow you to read in .xlsx files and other formats, however in this course we will only use .csv files. First, we will create an object called dat that contains the data in the ahi-cesd.csv file. Then, we will create an object called info that contains the data in the participant-info.csv. dat &lt;- read_csv (&quot;ahi-cesd.csv&quot;) pinfo &lt;- read_csv(&quot;participant-info.csv&quot;) There is also a function called read.csv(). Be very careful NOT to use this function instead of read_csv() as they have different ways of naming columns. For the portfolio tasks, unless your results match our exactly you will not get the marks which means you need to be careful to use the right functions. 2.2.6 Activity 4: Check yo’ data You should now see that the objects dat and pinfo have appeared in the environment pane. Whenever you read data into R you should always do an initial check to see that your data looks like you expected. There are several ways you can do this, try them all out to see how the results differ. In the environment pane, click on dat and pinfo. This will open the data to give you a spreadsheet-like view (although you can’t edit it like in Excel) In the environment pane, click the small blue play button to the left of dat and pinfo. This will show you the structure of the object information including the names of all the variables in that object and what type they are (also see str(pinfo)) Use summary(pinfo) Use head(pinfo) Just type the name of the object you want to view, e.g., dat. 2.2.7 Activity 5: Join the files together We have two files, dat and info but what we really want is a single file that has both the data and the demographic information about the participants. R makes this very easy by using the function inner_join(). Remember to use the help function ?inner_join if you want more information about how to use a function and to use tab auto-complete to help you write your code. The below code will create a new object all_dat that has the data from both dat and pinfo and it will use the columns id and intervention to match the participants’ data. Run this code and then view the new dataset using one of the methods from Activity 4. all_dat &lt;- inner_join(x = dat, # the first table you want to join y = pinfo, # the second table you want to join by = &#39;id&#39;, &#39;intervention&#39;) # columns the two tables have in common 2.2.8 Activity 6: Pull out variables of interest Our final step is to pull our variables of interest. Very frequently, datasets will have more variables and data than you actually want to use and it can make life easier to create a new object with just the data you need. In this case, the file contains the responses to each individual question on boh the AHI scale and the CESD scale as well as the total score (i.e., the sum of all the individual responses). For our analysis, all we care about is the total scores, as well as the demographic information about participants. To do this we use the select() function to create a new object named summarydata. summarydata &lt;- select(.data = all_dat, # name of the object to take data from ahiTotal, cesdTotal, sex, age, educ, income, occasion,elapsed.days) # all the columns you want to keep Run this code and then run head(summarydata). If everything has gone to plan it should look something like this: ahiTotal cesdTotal sex age educ income occasion elapsed.days 32 50 1 46 4 3 5 182.025 34 49 1 37 3 2 2 14.192 34 47 1 37 3 2 3 33.034 35 41 1 19 2 1 0 0.000 36 36 1 40 5 2 5 202.097 37 35 1 49 4 1 0 0.000 Finally, try knitting the file to HTML. And that’s it, well done! Remember to save your Markdown in your Lab folders and make a note of any mistakes you made and how you fixed them. You have started on your journey to become a confident and competent member of the open scientific community! There is no portfolio assessment this week, instead, use the time to get comfortable with what we’ve covered already and revise the activities and support materials presented so far if needed. If you’re feeling comfortable with R, you can work your way through this book at your own pace or push yourself by using the additional resources highlighted in Programming Basics. 2.3 Debugging tips When you downloaded the files from Moodle did you save the file names exactly as they were originally? If you download the file more than once you will find your computer may automatically add a number to the end of the filename. data.csv is not the same as data(1).csv. Pay close attention to names! Have you used the exact same object names as we did in each activity? Remember, name is different to Name. In order to make sure you can follow along with this book, pay special attention to ensuring you use the same object names as we do. Have you used quotation marks where needed? Have you accidentally deleted any backticks (```) from the beginning or end of code chunks? 2.4 Test yourself When loading in a .csv file, which function should you use? read_csv() read.csv() Explain This Answer Remember, in this course we use read_csv() and it is important for the portfolio assignment that you use this function otherwise you may find that the variable names are slightly different and you won’t get the marks The function inner_join() takes the arguments x, y, by. What does by do? Specifies the first table to join Specifies the second table to join Specifies the column to join by that both tables have in common What does the function select() do? Keeps only the observations you specify Keeps only the variables you specify Keeps only the objects you specify "],
["rm1-lab-2.html", "Chapter 3 RM1: Lab 2 3.1 Pre-class activities 3.2 In-class activities 3.3 Debugging tips 3.4 Test yourself", " Chapter 3 RM1: Lab 2 In this lab we move on from reading data in, joining tibbles and selecting variables of interest to becoming familiar with the Wickham 6 and the functionality of tidyverse. Practice is an important part of PsyTeachR so we ask students to repeat tasks across the semester in the pre-class, in-class and portfolio assessments. This principle is carried over into the student’s independent study practices. We encourage students to do prep independently but also to use the online help forums and to attend practice sessions where they can discuss and work on R in groups with GTAs present to guide and support if needed. This is an important part of our community building and creating a safe space to practice, make mistakes and develop skills. 3.1 Pre-class activities 3.1.1 Data wrangling and the tidyverse Data comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g. a subject), and each column stands for a different variable (e.g. a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place. It may surprise you to learn that scientists actually spend far more of time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80% of time spent on data analysis involves such data preparation tasks (Dasu &amp; Johnson, 2003)! Many people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand. In short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible. In this lesson you will see how data science skills will allow you to efficiently get answers to nearly any question you might want to ask about your data. By learning how to properly make your computer do the hard and boring work for you, you can focus on the bigger issues. 3.1.1.1 Tidyverse Tidyverse (https://www.tidyverse.org/) is a collection of R packages created by world-famous data scientist Hadley Wickham. Tidyverse contains six core packages: dplyr, tidyr, readr, purrr, ggplot2, and tibble. Last week when you typed library(tidyverse) into R, you will have seen that it loads in all of these packages in one go. Within these six core packages, you should be able to find everything you need to wrangle and visualise your data. In this lab, we are going to focus on the dplyr package, which contains six important functions: select() Include or exclude certain variables (columns) filter() Include or exclude certain observations (rows) mutate() Create new variables (columns) arrange() Change the order of observations (rows) group_by() Organize the observations into groups summarise() Derive aggregate variables for groups of observations These six functions are known as ’single table verbs’ because they only operate on one table at a time. Although the operations of these functions may seem very simplistic, it’s amazing what you can accomplish when you string them together: Hadley Wickham has claimed that 90% of data analysis can be reduced to the operations described by these six functions. 3.1.2 The babynames database To demonstrate the power of the six dplyr verbs, we will use them to work with the babynames data from the babynames package. The babynames dataset has historical information about births of babies in the U.S. 3.1.3 Activity 1: Set-up Do the following. If you need help, consult Programming Basics and RM1: Lab 1. Open R Studio and set the working directory to your Lab 2 folder. Open a new R Markdown document and save it in your working directory. Call the file “Pre-class 2”. If you are working on your own computer, install the package babynames. Remember, never install packages if you are working on a university computer. The university computers will already have this package installed. Delete the default R Markdown welcome text and insert a new code chunk that loads the packages tidyverse and babynames using library(). library(tidyverse) library(babynames) 3.1.4 Activity 2: Look at the data The package babynames contains an object of the same name that contains all the data about babynames. We can view a preview of this dataset by typing babynames in to the console. You should see the following output: babynames ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 ## 7 1880 F Ida 1472 0.0151 ## 8 1880 F Alice 1414 0.0145 ## 9 1880 F Bertha 1320 0.0135 ## 10 1880 F Sarah 1288 0.0132 ## # ... with 1,924,655 more rows The first line tells us that the object we are looking at is in fact a tibble with information on five variables with over 1.9 million rows. Yes, this dataset contains 1.8 million observations. Interested in analyzing these data by hand? No thanks! A tibble is basically a data frame presenting a two dimensional array of your data. Each row in the table represents data about births for a given name and sex in a given year. The variables are: variable type description year double (numeric) year of birth sex character recorded sex of baby (F = female, M = male) name character forename given to baby n integer number of babies given that name prop double (numeric) proportion of all babies of that sex The first row of the table tells us that in the year 1880, there were 7065 baby girls born in the U.S. who were given the name Mary, and this accounted for about 7% of all baby girls. 3.1.5 Activity 3: Your first plot Type the code below into a new code chunk and run it. The code might not make much sense to you right now, but don’t worry about not understanding it yet! The point is show you how much you can accomplish with very little code. The code creates a graph showing the popularity of four girl baby names - Alexandra, Beverly, Emily, and Kathleen - from 1880 to 2014. You should see Figure 3.1 appear, which shows the proportion of each name across different years - you can plug in different names if you like and see how the plot changes. dat &lt;- babynames %&gt;% filter(name %in% c(&quot;Emily&quot;,&quot;Kathleen&quot;,&quot;Alexandra&quot;,&quot;Beverly&quot;), sex==&quot;F&quot;) ggplot(data = dat,aes(x = year,y = prop, colour=name))+ geom_line() Figure 3.1: Proportion of four baby names from 1880 to 2014 3.1.6 Activity 4: Selecting variables of interest There are two numeric measurements of name popularity, prop (the proportion of all babies with each name) is probably more useful than n (total number of babies with that name), because it takes into account that different numbers of babies are born in different years. Just like in the RM1: Lab 1 in-class activity, if we wanted to create a dataset that only includes certain variables, we can use the select() function from the dplyr package. Run the below code to only select the columns year, sex, name and prop. select(.data = babynames, # the object you want to select variables from year, sex, name, prop) # the variables you want to select ## # A tibble: 1,924,665 x 4 ## year sex name prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1880 F Mary 0.0724 ## 2 1880 F Anna 0.0267 ## 3 1880 F Emma 0.0205 ## 4 1880 F Elizabeth 0.0199 ## 5 1880 F Minnie 0.0179 ## 6 1880 F Margaret 0.0162 ## 7 1880 F Ida 0.0151 ## 8 1880 F Alice 0.0145 ## 9 1880 F Bertha 0.0135 ## 10 1880 F Sarah 0.0132 ## # ... with 1,924,655 more rows Alternatively, you can also tell R which variables you don’t want, in this case, rather than telling R to select year, sex, name and prop, we can simply tell it to drop the column n using the minus sign - before the variable name. select(.data = babynames, -n) ## # A tibble: 1,924,665 x 4 ## year sex name prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1880 F Mary 0.0724 ## 2 1880 F Anna 0.0267 ## 3 1880 F Emma 0.0205 ## 4 1880 F Elizabeth 0.0199 ## 5 1880 F Minnie 0.0179 ## 6 1880 F Margaret 0.0162 ## 7 1880 F Ida 0.0151 ## 8 1880 F Alice 0.0145 ## 9 1880 F Bertha 0.0135 ## 10 1880 F Sarah 0.0132 ## # ... with 1,924,655 more rows Note that select() does not change the original tibble, but makes a new tibble with the specified columns. If you don’t save this new tibble to an object, it won’t be saved. If you want to keep this new dataset, create a new object. When you run this code, you will see your new tibble appear in the environment pane. new_dat &lt;- select(.data = babynames, -n) 3.1.7 Activity 5: Arranging the data The function arrange() will sort the rows in the table according to the columns you supply. Try running the following code: arrange(.data = babynames, # the data you want to sort name) # the variable you want to sort by ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2007 M Aaban 5 0.00000226 ## 2 2009 M Aaban 6 0.00000283 ## 3 2010 M Aaban 9 0.00000439 ## 4 2011 M Aaban 11 0.00000542 ## 5 2012 M Aaban 11 0.00000543 ## 6 2013 M Aaban 14 0.00000694 ## 7 2014 M Aaban 16 0.00000783 ## 8 2015 M Aaban 15 0.00000736 ## 9 2016 M Aaban 9 0.00000446 ## 10 2017 M Aaban 11 0.0000056 ## # ... with 1,924,655 more rows The data are now sorted in ascending alphabetical order by name. The default is to sort in ascending order. If you want it descending, wrap the name of the variable in the desc() function. For instance, to sort by year in descending order, run the following code: arrange(babynames,desc(year)) ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2017 F Emma 19738 0.0105 ## 2 2017 F Olivia 18632 0.00994 ## 3 2017 F Ava 15902 0.00848 ## 4 2017 F Isabella 15100 0.00805 ## 5 2017 F Sophia 14831 0.00791 ## 6 2017 F Mia 13437 0.00717 ## 7 2017 F Charlotte 12893 0.00688 ## 8 2017 F Amelia 11800 0.00629 ## 9 2017 F Evelyn 10675 0.00569 ## 10 2017 F Abigail 10551 0.00563 ## # ... with 1,924,655 more rows You can also sort by more than one column. What do you think the following code will do? arrange(babynames, desc(year), desc(sex), desc(prop)) 3.1.8 Activity 6: Using filter to select observations We have previously used select() to select certain variables or columns, however, frequently you will also want to select only certain observations or rows, for example, only babies born after 1999, or only babies named “Mary”. You do this using the verb filter(). The filter() function is a bit more involved than the other verbs, and requires more detailed explanation, but this is because it is also extremely powerful. Here is an example of filter, can you guess what it will do? filter(.data = babynames, year &gt; 2000) The first part of the code tells the function to use the object babynames. The second argument, year &gt; 2000, is what is known as a Boolean expression: an expression whose evaluation results in a value of TRUE or FALSE. What filter() does is include any observations (rows) for which the expression evaluates to TRUE, and exclude any for which it evaluates to FALSE. So in effect, behind the scenes, filter() goes through the entire set of 1.8 million observations, row by row, checking the value of year for each row, keeping it if the value is greater than 2000, and rejecting it if it is less than 2000. To see how a boolean expression works, consider the code below: years &lt;- 1996:2005 years years &gt; 2000 ## [1] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE You can see that the expression years &gt; 2000 returns a logical vector (a vector of TRUE and FALSE values), where each element represents whether the expression is true or false for that element. For the first five elements (1996 to 2000) it is false, and for the last five elements (2001 to 2005) it is true. Here are the most commonly used Boolean expressions. Operator Name is TRUE if and only if A &lt; B less than A is less than B A &lt;= B less than or equal A is less than or equal to B A &gt; B greater than A is greater than B A &gt;= B greater than or equal A is greater than or equal to B A == B equivalence A exactly equals B A != B not equal A does not exactly equal B A %in% B in A is an element of vector B If you want only those observations for a specific name (e.g., Mary), you use the equivalence operator ==. Note that you use double equal signs, not a single equal sign. filter(babynames, name == &quot;Mary&quot;) ## # A tibble: 268 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 M Mary 27 0.000228 ## 3 1881 F Mary 6919 0.0700 ## 4 1881 M Mary 29 0.000268 ## 5 1882 F Mary 8148 0.0704 ## 6 1882 M Mary 30 0.000246 ## 7 1883 F Mary 8012 0.0667 ## 8 1883 M Mary 32 0.000284 ## 9 1884 F Mary 9217 0.0670 ## 10 1884 M Mary 36 0.000293 ## # ... with 258 more rows If you wanted all the names except Mary, you use the ‘not equals’ operator: filter(babynames, name!=&quot;Mary&quot;) ## # A tibble: 1,924,397 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Anna 2604 0.0267 ## 2 1880 F Emma 2003 0.0205 ## 3 1880 F Elizabeth 1939 0.0199 ## 4 1880 F Minnie 1746 0.0179 ## 5 1880 F Margaret 1578 0.0162 ## 6 1880 F Ida 1472 0.0151 ## 7 1880 F Alice 1414 0.0145 ## 8 1880 F Bertha 1320 0.0135 ## 9 1880 F Sarah 1288 0.0132 ## 10 1880 F Annie 1258 0.0129 ## # ... with 1,924,387 more rows And if you wanted names from a defined set - e.g., names of British queens - you can use %in%: filter(babynames, name %in% c(&quot;Mary&quot;,&quot;Elizabeth&quot;,&quot;Victoria&quot;)) ## # A tibble: 772 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Elizabeth 1939 0.0199 ## 3 1880 F Victoria 93 0.000953 ## 4 1880 M Mary 27 0.000228 ## 5 1880 M Elizabeth 9 0.0000760 ## 6 1881 F Mary 6919 0.0700 ## 7 1881 F Elizabeth 1852 0.0187 ## 8 1881 F Victoria 117 0.00118 ## 9 1881 M Mary 29 0.000268 ## 10 1882 F Mary 8148 0.0704 ## # ... with 762 more rows This gives you data for the names in the vector on the right hand side of %in%. You can always invert an expression to get its opposite. So, for instance, if you instead wanted to get rid of all Marys, Elizabeths, and Victorias you would use the following: filter(babynames, !(name %in% c(&quot;Mary&quot;,&quot;Elizabeth&quot;,&quot;Victoria&quot;))) ## # A tibble: 1,923,893 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Anna 2604 0.0267 ## 2 1880 F Emma 2003 0.0205 ## 3 1880 F Minnie 1746 0.0179 ## 4 1880 F Margaret 1578 0.0162 ## 5 1880 F Ida 1472 0.0151 ## 6 1880 F Alice 1414 0.0145 ## 7 1880 F Bertha 1320 0.0135 ## 8 1880 F Sarah 1288 0.0132 ## 9 1880 F Annie 1258 0.0129 ## 10 1880 F Clara 1226 0.0126 ## # ... with 1,923,883 more rows You can include as many expressions as you like as additional arguments to filter() and it will only pull out the rows for which all of the expressions for that row evaluate to TRUE. For instance, filter(babynames, year &gt; 2000, prop &gt; .01) will pull out only those observations beyond the year 2000 that represent greater than 1% of the names for a given sex; any observation where either expression is false will be excluded. This ability to string together criteria makes filter() a very powerful member of the Wickham Six. 3.1.9 Activity 7: Creating new variables Sometimes we need to create a new variable that doesn’t exist in our dataset. For instance, we might want to figure out what decade a particular year belongs to. To create new variables, we use the function mutate(). Note that if you want to save this new column, you need to save it to an object. Here, you are mutating a new column and attaching it to the new_dat object you created in Activity 4. new_dat &lt;- mutate(.data = babynames, # the tibble you want to add a colum to decade = floor(year/10) *10) # new column name = what you want it to contain new_dat ## # A tibble: 1,924,665 x 6 ## year sex name n prop decade ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 1880 ## 2 1880 F Anna 2604 0.0267 1880 ## 3 1880 F Emma 2003 0.0205 1880 ## 4 1880 F Elizabeth 1939 0.0199 1880 ## 5 1880 F Minnie 1746 0.0179 1880 ## 6 1880 F Margaret 1578 0.0162 1880 ## 7 1880 F Ida 1472 0.0151 1880 ## 8 1880 F Alice 1414 0.0145 1880 ## 9 1880 F Bertha 1320 0.0135 1880 ## 10 1880 F Sarah 1288 0.0132 1880 ## # ... with 1,924,655 more rows In this case, you are creating a new column decade which has the decade hateach year appears in. This is calculated using the command decade = floor(year/10)*10. 3.1.10 Activity 8: Grouping and summarising Most quantitative analyses will require you to summarise your data somehow, for example, by calculating the mean, median or a sum total of your data. You can perform all of these operations using the function summarise(). First, let’s use the object dat that just has the data for the four girls names, Alexandra, Beverly, Emily, and Kathleen. To start off, we’re simply going to calculate the total number of babies across all years that were given one of these four names. It’s useful to get in the habit of translating your code into full sentences to make it easier to figure out what’s happening. You can read the below code as “run the function summarise using the data in the object dat to create a new variable named total that is the result of adding up all the numbers in the column n”. summarise(.data = dat, # the data you want to use total = sum(n)) # result name = operation ## # A tibble: 1 x 1 ## total ## &lt;int&gt; ## 1 2161374 summarise() becomes even more powerful when combined with the final dplyr function, group_by(). Quite often, you will want to produce your summary statistics broken down by groups, for examples, the scores of participants in different conditions, or the reading time for native and non-native speakers. There are two ways you can use group_by(). First, you can create a new, grouped object. group_dat &lt;- group_by(.data = dat, # the data you want to group name) # the variable you want to group by If you look at this object in the viewer, it won’t look any different to the original dat, however, the underlying structure has changed. Let’s run the above summarise code again, but now using the grouped data. summarise(.data = group_dat, total = sum(n)) ## # A tibble: 4 x 2 ## name total ## &lt;chr&gt; &lt;int&gt; ## 1 Alexandra 231364 ## 2 Beverly 376914 ## 3 Emily 841491 ## 4 Kathleen 711605 summarise() has performed exactly the same operation as before - adding up the total number in the column n - but this time it has done is separately for each group, which in this case was the variable name. You can request multiple summary calculations to be performed in the same function. For example, the following code calculate the mean and median number of babies given each name every year. summarise(group_dat, mean_year = mean(n), median_year = median(n)) ## # A tibble: 4 x 3 ## name mean_year median_year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alexandra 1977. 192 ## 2 Beverly 3089. 710. ## 3 Emily 6098. 1392. ## 4 Kathleen 5157. 3098 You can also add multiple grouping variables. For example, the following code groups new_dat by sex and decade and then calculates the summary statistics to give us the mean and median number of male and female babies in each decade. group_new_dat &lt;- group_by(new_dat, sex, decade) summarise(group_new_dat, mean_year = mean(n), median_year = median(n)) ## # A tibble: 28 x 4 ## # Groups: sex [2] ## sex decade mean_year median_year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F 1880 111. 13 ## 2 F 1890 128. 13 ## 3 F 1900 131. 12 ## 4 F 1910 187. 12 ## 5 F 1920 211. 12 ## 6 F 1930 214. 12 ## 7 F 1940 262. 12 ## 8 F 1950 288. 13 ## 9 F 1960 235. 12 ## 10 F 1970 147. 11 ## # ... with 18 more rows 3.1.11 Activity 9: Pipes The final activity for this pre-lab essentially repeats what we’ve already covered but in a slightly different way. In the previous activities, you created new objects with new variables or groupings and then you called summarise() on those new objects in separate lines of code. As a result, you had mulitple objects in your environment pane and you need to make sure that you keep track of the different names. Instead, you can use pipes. Pipes are written as %&gt;%and they should be read as “and then”. Pipes allow you to string together ‘sentences’ of code into ‘paragraphs’ so that you don’t need to create intermediary objects. Again, it is easier to show than tell. The below code does exactly the same as all the code we wrote above but it only creates one object. pipe_summary &lt;- mutate(babynames, decade = floor(year/10) *10) %&gt;% filter(name %in% c(&quot;Emily&quot;,&quot;Kathleen&quot;,&quot;Alexandra&quot;,&quot;Beverly&quot;), sex==&quot;F&quot;) %&gt;% group_by(name, decade) %&gt;% summarise(mean_decade = mean(n)) The reason that this function is called a pipe is because it ‘pipes’ the data through to the next function. When you wrote the code previously, the first argument of each function was the dataset you wanted to work on. When you use pipes it will automatically take the data from the previous line of code so you don’t need to specify it again. When learning to code it can be a useful practice to read your code ‘out loud’ in proper English to help you understand what it is doing. You can read the code above as “create a new variable called decade AND THEN only keep the names Emily, Kathleen, Alexandra and Beverly that belong to female babies AND THEN group the dataset by name and decade AND THEN calculate the mean number of babies with each name per decade.” Try doing this each time you write a new bit of code. Some people find pipes a bit tricky to understand from a conceptual point of view, however, it’s well worth learning to use them as when your code starts getting longer they are much more efficient and mean you have to write less code which is always a good thing! 3.2 In-class activities 3.2.1 Data-Wrangling: A Key Skill One of the key skills in an researcher’s toolbox is the ability to work with data. When you run an experiment you get lots of data in various files. For instance, it is not uncommon for an experimental software to create a new file for every participant you run and for each participant’s file to contain numerous columns and rows of data, only some of which are important. Being able to wrangle that data, manipulate it into different layouts, extract the parts you need, and summarise it, is one of the most important skills we will help you learn. Over this course you will develop your skills in working with data. This lab focuses on organizing data using the tidyverse package that you have read about in the pre-lab activities. Over the course, you will learn the main functions for data wrangling and how to use them, and we will use a number of different datasets to give you a wide range of exposure to what Psychology is about, and to reiterate that the same skills apply across different datasets. The skills don’t change, just the data! There are some questions to answer as you go along to test your skills: use the example code as a guide and the solutions are at the bottom. Remember to be pro-active in your learning, work together as a community, and if you get stuck use the cheatsheets. The key cheatsheet for this activity is the Data Transformation with dplyr. 3.2.2 Learning to Wrangle: Is there a Chastity Belt on Perception Nearly all data in research methods is stored in two-dimensional tables, either called data-frames, tables or tibbles. There are other ways of storing data that you will discover in time but mainly we will be using tibbles (if you would like more info, type vignette(\"tibble\") in the console). A tibble is really just a table of data with columns and rows of information. But within that table you can get different types of data, i.e. numeric, integer, and character. Type of Data Description Numeric Numbers including decimals Integer Numbers without decimals Character Tends to contain letters or be words Today we are going to be using data from this paper: Is there a Chastity Belt on Perception. You can read the full paper if you like, but we will summarise the paper for you. The paper asks, does your ability to perform an action influence your perception? For instance, does your ability to hit a tennis ball influence how fast you perceive the ball to be moving? Or to phrase another way, do expert tennis players perceive the ball moving slower than novice tennis players? This experiment does not use tennis players however, they used the Pong task: “a computerised game in which participants aim to block moving balls with various sizes of paddles”. A bit like a very classic retro arcade game. Participants tend to estimate the balls as moving faster when they have to block it with a smaller paddle as opposed to when they have a bigger paddle. You can read the paper to get more details if you wish but hopefully that gives enough of an idea to help you understand the wrangling we will do on the data. We have cleaned up the data a little to start with. Let’s begin! 3.2.3 Activity 1: Set-up If you need help with any of these steps, you should refer to the pre-lab activities: Download the data files from Moodle into your Lab 2 folder. Set the working directory to your Lab 2 folder. Open a new R Markdown document and save it in your working directory. Call the file “In-class 1”. Delete the default R Markdown welcome text and insert a new code chunk. Copy and paste the below code into this code chunk and then run the code. library(&quot;tidyverse&quot;) pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) 3.2.4 Activity 2: Look at your data Let’s have a look at the pong_data and see how it is organized. Type pong_data in your console window. In the dataset you will see that each row (observation) represents one trial per participant and that there were 288 trials for each of the 16 participants. The columns (variables) we have in the dataset are as follows: Variable Type Description Participant integer participant number JudgedSpeed integer speed judgement (1=fast, 0=slow) PaddleLength integer paddle length (pixels) BallSpeed integer ball speed (2 pixels/4ms) TrialNumber integer trial number BackgroundColor character background display colour HitOrMiss integer hit ball=1, missed ball=0 BlockNumber integer block number (out of 12 blocks) We will use this data to master our skills of the Wickham Six verbs, taking each verb in turn. You should refer to the explanations and example code in the pre-lab activities to help you complete these. There are 6 verbs to work through and after that we will briefly recap on two other functions before finishing with a quick look at pipes. Try each activity and ask your peers or your tutor if you need help. 3.2.5 Activity 3: select() Either by inclusion (telling R all the variables you want to keep) or exclusion (telling R which variables you want to drop), select only the Participant, PaddleLength, TrialNumber, BackgroundColor and HitOrMiss columns from pong_data and store it in a new object named select_dat. 3.2.6 Activity 4: Reorder the variables select() can also be used to reorder the columns in a table as the new table will display the variables in the order that you wrote them. Use select() to keep only the columns Participant, JudgedSpeed, BallSpeed, TrialNumber, and HitOrMiss but have them display in alphabetical order, left to right. Save this table in a new object named reorder_dat. 3.2.7 Activity 5: arrange() F Arrange the data by two variables: HitOrMiss (putting hits - 1 - first), and JudgedSpeed (fast judgement - 1 - first). Do not store this output in a new object. 3.2.8 Activity 6: filter() Use filter() to extract all Participants that had a fast speed judgement, for speeds 2, 4, 5, and 7, but missed the ball. Store this remaining data in a new object called pong_fast_miss Helpful Hint There are three parts to this filter so it is best to think about them individually and then combine them. Filter all fast speed judgements (JudgedSpeed) Filter for the speeds 2, 4, 5 and 7 (BallSpeed) Filter for all Misses (HitOrMiss) You could do this in three filters where each one uses the output of the preceeding one, or remember that filter functions can take more than one arguement. Also, because the JudgedSpeed and HitOrMiss are Integer you will need == instead of just =. The filter function is very useful but if used wrongly can give you very misleading findings. This is why it is very important to always check your data after you perform an action. Let’s say you are working in comparative psychology and have run a study looking at how cats, dogs and horses perceive emotion. Let’s say the data is all stored in the tibble animal_data and there is a column called animals that tells you what type of animal your participant was. Imagine you wanted all the data from just cats: filter(animal_data, animals == “cat”) Exactly! But what if you wanted cats and dogs? filter(animal_data, animals == “cat”, animals == “dog”) Right? Wrong! This actually says “give me everything that is a cat and a dog”. But nothing is a cat and a dog, that would be weird - like a dat or a cog. In fact you want everything that is either a cat or a dog, which is filter(animal_data, animals == “cat” | animals == “dog”)` The vertical line is the symbol for Or. So always pay attention to what you want and most importantly to what your code produces. \") 3.2.9 Activity 7: mutate() In the pre-lab, you learned how the mutate() function lets us create a new variable in our dataset. However, it also has another useful function in that it can be combined with recode() to create new columns with recoded values. For example, let’s add a new column to pong_data in which the background color is converted into numeric form where red will become 1, and blue will become 2. pong_data &lt;- mutate(pong_data, BackgroundColorNumeric = recode(BackgroundColor, &quot;red&quot; = 1, &quot;blue&quot; = 2)) The code here is is a bit complicated but: BackgroundColorNumeric is the name of your new column, BackgroundColor is the name of the old column and the one to take information from and 1 and 2 are the new codings of red and blue respectively character strings like “red” and “blue” are in quotation marks, numbers are not The mutate() function is also handy for making some calculations on or across columns in your data. For example, say you realise you made a mistake in your experiment where your participant numbers should be 1 higher for every participant, i.e. Participant 1 should actually be numbered as Participant 2, etc. You would do something like: pong_data &lt;- mutate(pong_data, Participant = Participant + 1) Note here that you are giving the new column the same name as the old column Participant. What happens here is that you are overwriting the old data with the new data! So watch out, mutate can create a new column or overwrite an existing column, depending on what you tell it to do! Imagine you realise there is a mistake in your dataset and that all your trial numbers are wrong. The first trial (trial number 1) was a practice so should be excluded and your experiment actually started on trial 2. Filter out all trials with the number 1 (TrialNumber column) from pong_data, Then use the mutate() function to recount all the remaining trial numbers, starting them at one again instead of two. Overwrite TrialNumber in pong_data with this new data. You can either do this in two separate steps and create a new object, or you can uses pipes %&gt;% and do it it one line of code. Helpful Hint Step 1. filter(TrialNumber does not equal 1) - remember to store this output in a variable? Step 2. mutate(TrialNumber = TrialNumber minus 1) 3.2.10 Activity 8: group_by() Group the data by BlockNumber and by BackgroundColor, in that order and save it in a new object named pong_data_group. View this new object by typing pong_data_groupinto the console. Enter the number of groups (i.e. a number) you get as a result: Helpful Hint It is the same procedure as this but with different column names: group_by(pong_data, HitOrMiss, BackgroundColor) The number of groups should be between the product of the number of background colors (red and blue) and the number of blocks (12). group_by() is incredibly useful as once the data is organised into groups you can then apply other functions (filter, arrange, mutate…etc.) to the groups within your data that you are interested in, instead of to the entire dataset. For instance, a common second step after group_by might be to summarise the data… 3.2.11 Activity 9: Summarising Data The summarise() function lets you calculate descriptive statistics for your data. For example, say you want to know the number of hits there were for different paddle lengths, or number of hits there were when the background color was red or blue. We will do this using pipes, to get you used to using them. Remember to try and read the code out loud and to pronounce %&gt;% as ‘and then’. Copy and paste the below code into a new code chunk and run the code. pong_data_hits&lt;- group_by(pong_data, BackgroundColor, PaddleLength) %&gt;% # first group the data summarise(total_hits = sum(HitOrMiss)) # and then create a new variable called total_hits summarise() has a range of internal functions that make life really easy, e.g. mean, sum, max, min, etc. See the dplyr cheatsheets for more examples. View pong_data_hits and enter the number of hits made with the small paddle (50) and the red color background in this box: Note: The name of the column within pong_data_hits is total_hits; this is what you called it in the above code. You could have called it anything you wanted but always try to use something sensible. Make sure to call your variables something you (and anyone looking at your code) will understand and recognize later (i.e. not variable1, variable2, variable3. etc.), and avoid spaces (use_underscores_never_spaces). After grouping data together using the group_by() function and then peforming a task on it, e.g. filter(), it can be very good practice to ungroup the data before performing another function. Forgetting to ungroup the dataset won’t always affect further processing, but can really mess up other things. Again just a good reminder to always check the data you are getting out of a function a) makes sense and b) is what you expect. 3.2.12 Two Other Useful Functions The Wickham Six verbs let you to do a lot of things with data, however there are thousands of other functions at your disposal. If you want to do something with your data that you are not sure how to do using these functions, do a Google search for an alternative function - chances are someone else has had the same problem and has a help guide. For example, two other functions to note are the bind_rows() function and the count() functions. The bind_rows() function is useful if you want to combine two tibbles together into one larger tibble that have the same column structure. For example: # a tibble of ball speeds 1 and 2 slow_ball&lt;- filter(pong_data, BallSpeed &lt; 3) # a tibble of ball speeds 6 and 7 fast_ball &lt;- filter(pong_data, BallSpeed &gt;= 6) # a combined tibble of extreme ball speeds extreme_balls &lt;- bind_rows(slow_ball, fast_ball) Finally, the count() function is a shortcut that can sometimes be used to count up the number of rows you have for groups in your data, without having to use the group_by() and summarise() functions. For example, in Task 6 we combined group_by() and summarise() to calculate how many hits there were based on background color and paddle length. Alternatively we could have done: count(pong_data, BackgroundColor, PaddleLength, HitOrMiss) The results are the same, just that in the count() version we get all the information, including misses, because we are just counting rows. In the summarise() method we only got hits because that was the effect of what we summed. So two different methods give similar answers - coding can be individualised and get the same result! 3.2.13 Pipes (%&gt;%) Finally, a quick recap on pipes. Here is an example of code that doesn’t use pipes to find how many hits there were with the large paddle length and the red background. # First we group the data accordingly, storing it in `pong_data_group` pong_data_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) # And then we summarise it, storing the answer in `total_hits` pong_data_hits &lt;- summarise(pong_data_group, total_hits = sum(HitOrMiss)) # And filter just the red, small paddle hits pong_data_hits_red_small &lt;- filter(pong_data_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 250) We can make our code even more efficient, using less code, by stringing our sequence of functions together using pipes. This would look like: # Same pipeline using pipes pong_data_hits_red_small &lt;- pong_data %&gt;% # take pong_data and then group_by(BackgroundColor, PaddleLength) %&gt;% # group by BackgroundColor and PaddleLength and then summarise(total_hits = sum(HitOrMiss)) %&gt;% # calculate the total number of hits and then filter(BackgroundColor == &quot;red&quot;, PaddleLength == 250) # only keep the data for the red large paddle One last point on pipes is that they can be written in a single line of code but it’s much easier to see what the pipe is doing if each function takes its own line. Every time you add a function to the pipeline, remember to add a %&gt;% first and note that when using separate lines for each function, the %&gt;% must appear at the end of the line and not the start of the next line. Compare the two examples below. The first won’t work but the second will because the second puts the pipes at the end of the line where they need to be! # Piped version that wont work data_arrange &lt;- pong_data %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) # Piped version that will work data_arrange &lt;- pong_data %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) Where piping becomes most useful is when we string a series of functions together, rather than using them as separate steps and having to save the data each time under a new variable name and getting ourselves all confused. In the non-piped version we have to create a new variable each time, for example, data, data_filtered, data_arranged, data_grouped, data_summarised just to get to the final one we actually want, which was data_summarised. This creates a lot of variables and tibbles in our environment and can make everything unclear and eventually slow down our computer. The piped version however uses one variable name, saving space in the environment, and is clear and easy to read. With pipes we skip unnecessary steps and avoid cluttering our environment. We have now learned a number of functions and verbs that you will need as the semester goes on. You will use them in the lab next week so be sure to go over these and try them out to make yourself more comfortable with them. If you have any questions please post them on the Moodle forum or the slack forum. Happy Wrangling! 3.2.14 Activity solutions Below you will find the solutions to the above questions. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 3.2.14.1 Activity 3 Solution Task 3 # To include variables: select_dat &lt;- select(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss) # To exclude variables: select_dat &lt;-select(pong_data, -JudgedSpeed, -BallSpeed, -BlockNumber) click the tab to see the solution 3.2.14.2 Activity 4 Solution Activity 4 reorder_dat &lt;- select(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber) click the tab to see the solution 3.2.14.3 Activity 5 Solution Task 2 arrange(pong_data, desc(HitOrMiss), desc(JudgedSpeed)) click the tab to see the solution 3.2.14.4 Activity 6 Solution Activity 6 pong_fast_miss&lt; - filter(pong_data, JudgedSpeed == 1, BallSpeed %in% c(&quot;2&quot;, &quot;4&quot;, &quot;5&quot;, &quot;7&quot;), HitOrMiss == 0) click the tab to see the solution 3.2.14.5 Activity 7 Solution Activity 7 4 # this is the solution if you used two separate steps pong_data_filt &lt;- filter(pong_data, TrialNumber &gt;= 2) # you can call this variable anything, as long as it makes sense to yourself and others pong_data &lt;- mutate(pong_data_filt, TrialNumber = TrialNumber - 1) # this is the solution if you used pipes pong_data&lt;- filter(pong_data, TrialNumber &gt;= 2) %&gt;% mutate(TrialNumber = TrialNumber - 1) click the tab to see the solution 3.2.14.6 Activity 8 Solution Task 5 pong_data_group &lt;- group_by(pong_data, BlockNumber, BackgroundColor) pong_data_group click the tab to see the solution 3.2.14.7 Activity 9 Solution Activity 9 pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) pong_data_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) pong_data_hits &lt;- summarise(pong_data_group, total_hits = sum(HitOrMiss)) # the answer should give 517 click the tab to see the solution 3.3 Debugging tips Make sure you have spelt the data file name exactly as it is shown. Spaces and everything. Do not change the name of the csv file, fix your code instead. If you have a different name for your file than someone else then your code is not reproducible. Remember when uploading data we use read_csv() which has an underscore, whereas the data file itself will have a dot in its name, filename.csv. Finally, check that the datafile is actually in the folder you have set as your working directory. 3.4 Test yourself What type of data would these most likely be: Male = Character Numeric Integer 7.15 = Character Numeric Integer 137 = Character Numeric Integer Explain These Answers There is a lot of different types of data and as well as different types of levels of measurements and it can get very confusing. It’s important to try to remember which is which because you can only do certain types of analyses on certain types of data and certain types of measurements. For instance, you can’t take the average of Characters just like you can’t take the average of Categorical data. Likewise, you can do any maths on Numeric data, just like you can on Interval and Ratio data. Integer data is funny in that sometimes it is Ordinal and sometimes it is Interval, sometimes you should take the median, sometimes you should take the mean. The main point is to always know what type of data you are using and to think about what you can and cannot do with them. Which of the Wickham Six would you use to sort columns from smallest to largest: select filter mutate arrange group_by summarise Which of the Wickham Six would you use to calculate the mean of a column: select filter mutate arrange group_by summarise Which of the Wickham Six would you use to remove certain observations - e.g. remove all males: select filter mutate arrange group_by summarise What does this line of code say? data %&gt;% filter() %&gt;% group_by() %&gt;% summarise(): take the data and then group it and then filter it and then summarise it take the data and then filter it and then group it and then summarise it take the data and then summarise it and then filter it and then group it take the data and then group it and then summarise it and then filter it "],
["rm1-lab-3.html", "Chapter 4 RM1: Lab 3 4.1 Pre-lab activities 4.2 In-class activities", " Chapter 4 RM1: Lab 3 4.1 Pre-lab activities This is the final lab of the first semester. In the pre-lab the focus is again on data wrangling skills and repetition of key functions they have covered in lab 1 and 2. The in-class introduces inferential analyses for the first time to coincide with their statistics lecture on chi-square. One of the key aims of this lab is to demonstrate that they have already done the difficult part of R - data wrangling - and that statistical analyses are frequently very easy to peform. We also introduce visualisations and ggplot2 so that there is a tangible output to their coding. 4.1.1 Data wrangling recap Last week we looked at using one-table Wickham verbs to filter, arrange, group_by, select, mutate and summarise. Now we will focus on working with data across two or more tables. The two main verbs we will practice adding to the Wickham six today are gather() and inner_join()and these will help you process your data for the mini-project. gather() allows us to transform a table from wide format to long format (more on this below). inner_join() allows us to combine two tables together based on common columns. A function is a tool that takes an input, performs some action, and gives an output. They are nothing more than that. If you think about it your toaster is a function: it takes bread as an input; it perfoms the action of heating it up (nicely sometimes; on both sides would be a luxury); and it gives an output, the toast. A good thing about the Wickham six functions is that they are nicely named as verbs to describe what they do - mutate() mutates (adds on a column); arrange() arranges columns, summarise() summarises, etc. In terms of remembering all the functions, the truth is you don’t have to know them all. However, through practice and repetition, you will quickly learn to remember which ones are which and what package they come from. Sort of like where to find your spoons in your kitchen - you don’t look in the fridge, and then the washing machine, and then the drawer. Nope, you learnt, by repetition, to look in the drawer first time. It’s the same with functions. Keep in mind that research methods is like a language in that the more you use it and work with it the more it makes sense. 4.1.2 Tidy Data We will use the most efficient format/layout of data which is known as Tidy Data. Any data in this format is easily processed through the tidyverse package. However, the data you work with will not always be formatted this way. If that happens then our first step is to put it into Tidy Data format. There are three fundamental rules defining Tidy Data: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell (i.e. no grouping two variables together, e.g. time/date in one cell). A cell is where any specific row and column meet; a single data point in a tibble is a cell. If you’ve worked with any kind of data before, particularly if you’ve used Excel, it’s very likely that you will have used wide format data. In Wide format, each participant’s data is all in one row with multiple columns for different data points. This means that the data set tends to be very wide and you will have as many rows as you have participants. This layout can be easy to read, howver, it makes programming quite difficult. Whilst Tidy Data can be conceptually more difficult to understand at first, it means you can manipulate your data in whatever way you want very easily. 4.1.3 Analysing the Autism Specturm Quotient (AQ) To continue building your data wrangling skills in this pre-lab you will tidy data from the Autism Spectrum Quotient (AQ) questionnaire. The AQ10 is a non-diagnostic short form of the AQ with only 10 questions per participant. It is a discrete scale and the higher a participant scores on the AQ10 the more autistic-like traits they are said to display. Anyone scoring 7 or above is recommended for further diagnosis. You can see an example of the AQ10 through this link: AQ10 Example. There are 66 participants and your goal in this pre-class activity is to find an AQ score for each of them through your data-wrangling skills. There are four data files to work with that you should download from Moodle: responses.csv containing the AQ survey responses to each of the 10 questions for the 66 participants qformats.csv containing information on how a question should be coded - i.e. forward or reverse coding scoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded pinfo.csv containing participant information such as Age, Sex and importantly ID number. csv stands for ‘comma separated variable’, and is a very basic way of transferring data. It really just stores numbers and text and nothing else. The great thing about being this basic is that it can be read by many different machines and does not need expensive licenses to open it. 4.1.4 Activity 1: Set-up Do the following. If you need help, consult Programming Basics and RM1: Lab 1. Open R Studio and set the working directory to your Lab 3 folder. Open a new R Markdown document and save it in your working directory. Call the file “Pre-class 3”. Download the four .csv files from Moodle and save them in your Lab 3 folder. Make sure that you do not change the file names at all. Delete the default R Markdown welcome text and insert a new code chunk that loads the package tidyverse using the library() function. 4.1.5 Activity 2: Load in the data Now you need to load in the .csv datafiles using the read_csv() function and save them as variables in the environment. For example, to load in the responses file we would type: responses &lt;- read_csv(&quot;responses.csv&quot;) Add the following lines of code to your Markdown and complete them to load in all four .csv datafiles. Use the above code as an example and name each variable the same as its original filename (minus the .csv part), again as above, e.g. responses.csv gets saved as responses. Remember to run the lines so that the data loaded in and is stored in your environment. responses &lt;- read_csv() # survey responses qformats &lt;- # question formats scoring &lt;- # scoring info pinfo &lt;- # participant information 4.1.6 Activity 3: Look at your data Now that we have the data loaded in it is always best to have a look at the data to get an idea of its layout. We showed you ways of doing this before, but you can also use the glimpse() or View() functions in your Console window and put the name of the data between the brackets to see how it is arranged. Don’t add these to your script though they are just one-offs for testing. Have a look at the data in responses to see if you think it is Tidy or not and answer the following question: The data in responses is in Tidy Wide format Explain This Answer The responses tibble is far from being tidy; each row represents multiple observations from the same participant, i.e. each row shows responses to multiple questions and there are the same number of rows as there are particpants (66) - wide format. Remember we want the data in tidy format as described above. 4.1.7 Activity 4: Gathering Data. We now have all the data we need loaded in, but in order to make it easier for us to get the AQ score for each participant, we need to change the layout of the responses tibble to Tidy Data using the gather() function. Copy the below code line to a new code chunk and run it. rlong &lt;- gather(data = responses, # the dataset we want to work on key = Question, # the name of the variable that will store what is currently the names of each column (question numbers) value = Response, # the name of the new column that will store the values (data points) Q1:Q10) # the columns we want to gather together In case you are wondering if we wanted to go back the way, and ungather the data we just gathered, we would use the spread() function: e.g. rwide &lt;- spread(rlong, Questions, Response). But we do not want to do that here so let’s not add this to the code. In the code above we have used the notation Q1:Q10. This means ’select all the columns from Q1 to Q10. We could have written out the name of each column individually, for example Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10 but obviously it is much easier to use the shorthand notation. You must be careful though to know what you are selecting. R isn’t clever enough to realise that what you want is all the Question columns - it would take any and all columns that exist between Q1 and Q10. This means that if your dataset is out of order you may end up selecting columns you didn’t mean to. Always look at your data and make sure you know the layout. Look at the new dataset rlong. Compare it to the original dataset responses and try to understand how they relate to each other. 4.1.8 Activity 5: Combining data Now the responses data is in tidy format, you are closer to being able to calculate an AQ score for each person. However, you still need some extra information: Is the question is reverse or forward scored (i.e., is strongly agree a positive or negative response)? This information is found in qformats How many points are given to give a specific response? This informatio is found in scoring. This is a typical analysis situation where different information is in different tables and you need to join them altogether. Both these pieces of information are contained in qformats and scoring respectively, but we want to join them to responses to create one informative tidy table with all the information we need. We can do this through the function inner_join(); a function to combine information in two tibbles using a column common to both tibbles. Replace the NULL values in the below code with the necessary variable names to join rlong1 and qformats by Question. If you need extra help, revisit Lab 1 In-class Activity 4 - you used the same function then! You can also check the solutions for the answer (but make sure you try yourself first). rlong2 &lt;- inner_join(x = NULL, y = NULL, by = &quot;NULL&quot;) Now view rlong2. You have matched each question with its scoring format, forward or reverse. A lot of questionnaires have some questions that are Forward scored and some questions that are Reverse scored. What does this mean? Imagine a situation where your options in replying to a question are: 1 - extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely disagree. In a forward-scoring question you would get 1 point for extremely agree, 2 for agree, 3 for neutral, etc. In a reverse scoring question you would get 5 for extremely agree, 4 for agree, 3 for neutral, etc. The reasoning behind this shift is that sometimes agreeing or disagreeing might be more favourable depending on how the question is worded. Secondly, sometimes these questions are used just to catch people out - imagine if you had two similar questions where one has the reverse meaning of the other. In this scenario, people should respond opposites. If they respond the same then they might not be paying attention. 4.1.9 Activity 6: Combining more data Now you need to combine the information in our new table, rlong2, with the scoring table so you know how many points to attribute each question based on the answer the participant gave, and whether the question was forward or reverse coded. Again, you can use the inner_join() function, but this time the common columns found in rlong2 and scoring are QFormat and Response. To combine by two columns you just write them in sequence as shown below. **Note: when there is more than one common column between two tibbles you are joining, it is best to combine by all the columns to avoid repeat columns names in the new tibble. Copy the below line into a code chunk, run it, and then view the new object. # combine rows in rlong2 and scoring based on QFormat and Response rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 4.1.10 Activity 7: Calculating the AQ Scores. You have now created rscores which has information on how each participant responded to each question and how each question should be coded and scored, all within the one tibble. All you need now is to sum the scores for each participant to get their AQ score. Based on your knowledge from last week, copy the below line into your code and replace the NULLs to obtain individual aq_scores for each participant. Save your Markdown and knit it to make sure all your code works. aq_scores &lt;- rscores %&gt;% group_by(NULL) %&gt;% # how will you group individual participants? summarise(AQ = sum(NULL)) # which column will you sum to obtain AQ scores? Helpful Hint Each participant could be grouped by their Id. If we summed up the value for each Score we might get a full AQ Score for each particpipant. 4.1.11 Activity 8: One aast thing on pipes You now have a complete code to load in your data, convert it to Tidy, combine the tables and calculate an AQ score for each participant. But, if you look at it, some of your code could be more efficient by using pipes. Go back through your code and try to rewrite it using pipes %&gt;% so that it is as efficient as possible. Helpful Hint At any point where the first argument of your function is the name of a variable created before that line, there is a good chance you could have used a pipe! Here are all the bits of this code that could be piped together into one chain: rlong &lt;- gather(responses, Question, Response, Q1:Q10) rlong2 &lt;- inner_join(rlong, qformats, \"Question\") rscores &lt;- inner_join(rlong2, scoring, c(\"QFormat\", \"Response\")) aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) You have now recapped one-table and two-table verbs. These are great to know as for example, in the above, it actually only took a handful of reproducible steps to get from messy data to tidy data; could you imagine doing this by hand in Excel through cutting and pasting? Not to mention the mistakes you could make! If you have any questions, please post them on the slack forum. 4.1.12 Activity solutions Below you will find the solutions to the above questions. Only look at them after giving the questions a good try and trying to find help on Google or Slack about any issues. 4.1.12.1 Activity 2 Activity 2 responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) scoring &lt;- read_csv(&quot;scoring.csv&quot;) pinfo &lt;- read_csv(&quot;pinfo.csv&quot;) Click the tab to see the solution 4.1.12.2 Activity 5 Solution Task 5 rlong2 &lt;- inner_join(x = rlong, y = qformats, by = &quot;Question&quot;) Click the tab to see the solution 4.1.12.3 Activity 7 Solution Task 7 aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% # group by the ID number in column Id summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores. Click the tab to see the solution 4.1.12.4 Activity 8 Activity 8 aq_scores2 &lt;- responses %&gt;% # take the data in `responses` and then gather(Question, Response, Q1:Q10) %&gt;% # gather up columns Q1 to Q10, put the column names in Question and the scores in Response and then inner_join(qformats, &quot;Question&quot;) %&gt;% # join with `qformats` and match the data by the column `Question` and then inner_join(scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) %&gt;% # join with `scoring` and match the data by the columns `Qformat` and `Response` and then group_by(Id) %&gt;% # group by participant ID and then summarise(AQ = sum(Score)) # calculate the total AQ score Click the tab to see the solution 4.1.13 Test yourself You want to gather the first three columns of a file called responses (Q1, Q2, Q3), put the question numbers in a column called Jam, the responses in a column called Strawberry, and store everything in a tibble called sandwich. Fill in the box with what you would write: Explain this answer sandwich &lt;- gather(data = responses, key = Jam, values = Strawberry, Q1:Q3) gather wants the data first, then the name of the new column to store the gathered column names, then the name of the new column to store the data, and then finally which columns to gather. Complete the sentence, the higher the AQ score… the less autistic-like traits displayed has no relation to autistic-like traits the more autistic-like traits displayed Type in the AQ score (just the number) of Participant ID No. 87: Type how many participants had an AQ score of 3 (again just the number): The cut-off for the AQ10 is usually said to be around 6 meaning that anyone with a score of more than 6 should be referred for diagnostic assessment. Type in how many participants we should refer from our sample: Explain this - I dont get these answers From the link above you can see that an appropriate citation for the AQ10 would be (Allison, Auyeung, and Baron-Cohen, (2012)) As mentioned, the higher the score on the AQ10 the more autistic-like traits a participant is said to show. You could do this by code with filter(aq_scores, Id == 87), which would give you a tibble of 1x2 showing the ID number and score. If you just wanted the score you could use pull() which we havent shown you that yet: filter(aq_scores, Id == 87) %&gt;% pull(AQ). The answer is an AQ score of 2. Same as above but changing the arguement of the filter. filter(aq_scores, AQ == 3) %&gt;% count(). The answer is 13. Remember you can do this by counting but the code makes it reproducible and accurate every time. You might make mistakes. filter(aq_scores, AQ &gt; 6) %&gt;% count() or filter(aq_scores, AQ &gt;= 7) %&gt;% count(). The answer is 6. 4.2 In-class activities 4.2.1 Data analysis We have spent the last few weeks focusing on the basics of R and data wrangling. You may think that the tasks we ask you to do in R will get harder as this course progresses but that isn’t true. The hardest part of learning R is at the very beginning, trying to learn the new terminology, figuring out how to load in data and wrangle it into the format you need. It may feel like you are still struggling so it’s worth reflecting on just how far you’ve come in less than 3 weeks. You can now: Understand what functions, arguments, objects, variables, and tibbles are Read data into R Tidy data into an appropriate format Calculate a range of descriptive statistics That’s amazing! Now we’re going to move on to performing some simple inferential analyses and creating a plot to visualise the data. 4.2.2 Reminders through association For the final lab of RM1, we will focus on chi-square that you covered in the lecture last week. For this lab, we’re going to use data from Rogers, T. &amp; Milkman, K. L. (2016). Reminders through association. Psychological Science, 27, 973-986. You can read the full paper online but the short version is that the authors looked at how people remember to follow through with the intention of doing something. Although there are lots of potential reasons (e.g., some people may lack the self-control resources), Rogers and Milkman (2016) propose that some people fail to follow through simply because they forget about their good intentions. If this is the case, the authors argue, then having visual reminders to follow through on their intentions may help people remember to keep them. For example, a person may choose to put a sticker for their gym on their car window, so that every time they get in the car they remember to go to the gym. In Study 1, participants took part in an unrelated experiment but at the start of the task they were asked to return a small stack of paper clips to the reception of the building at the end of the study and if they did so the researchers would donate $1 to a charity. They were then asked if they intended to do this. Those in the reminder-through-association (RTA) condition read “Thank you! To remind you to pick up a paper clip, an elephant statuette will be sitting on the counter as you collect your payment.” This message was followed by a picture of the elephant statuette. Those in the control condition simply read “Thank you!”. What we want to do is to run a chi-square analysis to determine whether those in the RTA condition were more likely to remember to return the paperclips than those in the control condition. 4.2.3 Activity 1: Set-up Do the following. If you need help, consult Programming Basics and RM1: Lab 1. Open R Studio and set the working directory to your Lab 3 folder. Open a new R Markdown document and save it in your working directory. Call the file “In-class 3”. Download the RTA_study1.csv file from Moodle and save it in your Lab 3 folder. Make sure that you do not change the file name at all. Delete the default R Markdown welcome text and insert a new code chunk that loads the package tidyverse using the library() function and loads the data into an object named intent_data using read_csv(). library(NULL) intent_data &lt;- read_csv(NULL) 4.2.4 Activity 2: Look at the data Using your preferred method, look at the data. It is a fairly simple data file that contains four variables for 87 participants: condition: this variable indicates which condition participants were in, 1 = reminder-through-association condition, 2 = control condition intend: this variable indicates whether particpants said they were intending to return the paperclips, 1 = yes, 0 = no actualdonate: this variable indicates whether participants actually ended up returning the paperclips and therefore donating to charity, 1 = yes, 0 = no id: this variable indicates the participant ID number 4.2.5 Activity 3: Wrangle and recode the data We need to do a little bit of wrangling to get our data into the format we need. First, we need to remove all the participants who said that they did not intend to return the paperclips (intend = 0) as we are only interested in whether people follow through on an intention. Second, to make the output easier to read, we’re going to recode condition to have text labels rather than numerical values. Use filter() to remove all participants who said that they did not intend to return the paperclips Use mutate() and recode() to recode the values in condition to make 1 = rta and 2 = control and the values in actualdonate to 1 = donated and 0 = no_donation. If you need help with this, consult Lab 3 in-class activity 7. You can do this in two separate steps, or you can use pipes. Regardless of how you do it, save the final output to an object named intent_recode. The solutions are at the end of this chapter but make sure you try it yourself and ask your peers and tutor for help first. intent_recode &lt;- Helpful hint You will need to put both sides of each recode argument (i.e., 1 and rta) in quotation marks, even though 1 and 2 are numbers, they actually represent categories rather than numerical data. intent_recode should have data from 77 participants and should look something like this: condition intend actualdonate id rta 1 donated 1 rta 1 donated 2 rta 1 donated 3 rta 1 donated 4 rta 1 donated 5 rta 1 donated 6 4.2.6 Activity 4: Descriptive statistics Next you need to calculate descriptive statistics. For frequency data these are simply counts so we can use the function count() rather than having to use summarise. We want to know how many participants are in each group (rta - donated, rta - didn’t donate, control - donated, control - didn’t donate) so we will need to use group_by to display the results for all combinations of condition and actualdonate. Replace the NULLs in the below code to calculate the number of participants in each category and save it to an object named intent_counts. intent_counts &lt;- intent_recode %&gt;% group_by(NULL, NULL) %&gt;% count() How many participants in the control condition didn’t donate? How many participants in the control condition donated? How many participants in the rta condition didn’t donate? How many participants in the rta condition donated? You may also want to calculate the percentage of people who donated in each condition, if so you can adapt the code like this: intent_percent &lt;- intent_recode %&gt;% group_by(condition, actualdonate) %&gt;% count() %&gt;% ungroup() %&gt;% # ungroups the code group_by(condition) %&gt;% # then groups it again but just by condition mutate(percent_condition = n/sum(n) * 100) condition actualdonate n percent_condition control donated 16 42.11 control no_donation 22 57.89 rta donated 29 74.36 rta no_donation 10 25.64 4.2.7 ggplot2() Now you have calculated how many participants are in each cell (or combination of the categories), however, it is also useful to create a visualisation of the data - the old saying is true, a picture is worth a thousand words. To make our data visualisations we’re going to use the package ggplot2() which was loaded as part of the tidyverse. ggplot() builds plots by combining layers (see Figure 4.1)). If you’re used to making plots in Excel this might seem a bit odd at first, however, it means that you can customise each layer and R is capable of making very complex and beautiful figures (this website gives you a good sense of what’s possible). Figure 4.1: ggplot2 layers from Field et al. (2012) 4.2.8 Activity 5: Bar plot We want to create a simple bar plot of our count data. Copy and paste the below code into a new R chunk and run it. The first line (or layer) sets up the base of the graph: the data to use and the aesthetics (what will go on the x and y axis, how the plot will be grouped). aes() can take both an x and y argument, however, with a bar plot you are just asking R to count the number of data points in each group so you don’t need to specify this. fill will separate the data into each level of the grouping variable and give it a different colour. In this case, there is a different coloured bar for each level of actualdonate. The next layer adds a geom or a shape, in this case we use geom_bar() as we want to draw a bar plot. position = \"dodge\" places the bars next to each other, rather than on top of each other. Try removing this argument and just running the code with geom_bar() to see what happens. ggplot(data = intent_recode, aes(x = condition, fill = actualdonate)) + geom_bar(position = &quot;dodge&quot;) Figure 4.2: Bar plot of RTA Study 1 data In R terms, ggplot2 is a fairly old package. As a result, the use of pipes wasn’t included when it was originally written. As you can see in the code above, the layers of the code are separated by + rather than %&gt;%. In this case, + is doing essentially the same job as a pipe - be careful not to confuse them. As you can see, the plot makes it much easier to visualise the data - participants in the RTA condition appear to have been more likely to remember to donate than those in the control condition. 4.2.9 Activity 6: Make the plot pretty As mentioned, ggplot2 allows you to customise all aspects of your plots, so let’s tidy ours up a little bit. We’re going to do the following: * Edit the labels on the x-axis, y-axis and fill * Change the colours of the bars by using scale_fill_manual() and specifying the colours we want in the values argument * Change the theme of the plot to change how it looks visually ggplot(data = intent_recode, aes(x = condition, fill = actualdonate)) + geom_bar(position = &quot;dodge&quot;) + scale_x_discrete(name = &quot;Condition&quot;, labels = c(&quot;Control&quot;, &quot;RTA&quot;)) + scale_y_continuous(name = &quot;Count&quot;) + scale_fill_manual(name = &quot;Behaviour&quot;, labels = c(&quot;Donated&quot;, &quot;Did not donate&quot;), values = c(&quot;blue&quot;, &quot;grey&quot;))+ theme_classic() (#fig:plot_edits)Prettier bar plot of RTA Study There are a few things to note about the code we just added on: * The first two lines are the same code as we used in Activity 4, what we’ve done now is add on extra layers. * If you use simple colour names then you are restricted in the options you can choose, however, if you want more flexiblity you can use hexadecimal colour codes, see here for more information. * There are multiple themes that you can apply. If you type theme_ the auto-complete will show you the options - try a few out and see which one you prefer. * If you want more information on any of these functions, remember you can look at the help documentation by typing ?function. 4.2.10 Activity 7: Chi-square So, let’s finally run that chi-square analysis to see whether our intuition from the plot holds up and there is a significant association between the grouping variables. As promised, the code is quite simple - copy and paste the below code into a new R chunk and run it. results &lt;- chisq.test(x = intent_recode$condition, # the first grouping variable y = intent_recode$actualdonate, # the second grouping variable correct = FALSE) # whether we want to apply the continuity correction results ## ## Pearson&#39;s Chi-squared test ## ## data: intent_recode$condition and intent_recode$actualdonate ## X-squared = 8.244, df = 1, p-value = 0.004089 This code looks a little different to code you’ve used up until this point as it comes from Base R. The x and y variables use the notation object$variable so our x variable could be read as \"use the variable condition from the object intent_recode. The reason that we chose not to apply the continuity correction is because this is what the analysis in the original paper did. What is the chi-square statistic? Is the p-value significant? Yes No What are the degrees of freedom for the test? Explain these answers The chi-square statistic is noted in the output as X-squared. Refer to the lecture for more information on how this number is calculated. The traditional cut-off for significance is p &lt; .05. This means that if your p-value is smaller than .05 there is a statistically significant association, that is, you would be unlikely to observe this pattern of data by chance if the null hypothesis was true. If p is larger than .05 it means that there is a higher probability that any difference you see would be likely to occur even if the null hypothesis was true. Pay attention to the decimal places, they make a huge difference! Degrees of freedom are noted as df in the output. Refer to the lecture for more information on what they are and how they are calculated. Go and find the results section in the original paper, do your numbers match the ones they report? 4.2.11 Activity 8: Additional analysis information You may have noticed that when you ran the chi-square an object appeared in the environment that saved the results of the analysis. This object is a list which is a bit different to the type of objects we’ve worked with so far. Lists don’t just contain one data table or a vector of numbers or characters, they can contain multiple different types of information and multiple different tables. We can see that our object results is a list of 9, which means it has 9 components. Click on results in the environment pane to view the contents of the list (you could also type str(list)). Figure 4.3: Contents of a list Each of these components can be viewed separately using the same object$variable notation we used above. For example, if we wanted to view the observed frequencies (refer to the lecture), we would run the following code: results$observed donated no_donation control 16 22 rta 29 10 4.2.12 Assumption checks The assumptions for chi-square are as follows: The data in the cells should be frequencies, or counts of cases rather than percentages or some other transformation of the data. The levels (or categories) of the variables are mutually exclusive. That is, a particular participant fits into one and only one group of each of the variables. Each subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used. The study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child. There are 2 variables, and both are measured as categories, usually at the nominal level. While Chi-square has no rule about limiting the number of cells (by limiting the number of categories for each variable), a very large number of cells (over 20) can make it difficult to meet assumption #6 below, and to interpret the meaning of the results. The expected cell frequencies should be greater than 5. 4.2.13 Activity 9: Check the expected frequencies We know that assumptions 1-5 have been met because we know the design of the study and the type of data we have fits these criteria. The final assumption we need to test is that all expected frequencies are greater than 5. Using the same object$variable code as in Activity 7, view the expected frequencies Does the data meet assumption 6? 5\"]'> Yes - all expected frequencies are &gt; 5 No - one or more expected frequencies are &lt; 5 4.2.14 Activity 10: Writing up Now that you’ve run all of the analyses you can use inline coding to help you write up your results. This isn’t something you’re going to be tested on in this course but it’s a really cool feature of Markdown so for each statistical test we’ll show you the code that does it so that you can use it in the future if you wanted to. We’re going to replicate the exact write-up of the results from the original paper. In the whitespace in your Markdown document, copy and paste the following (do not change anything): Those in the reminder-through-association condition performed the intended behavior at a significantly higher rate (`r round(pluck(intent_percent$percent_condition, 3),0)`%, `r pluck(intent_percent$n, 3)` out of `r pluck(intent_percent$n, 3) + pluck(intent_percent$n, 4)`) than did those in the control condition (`r round(pluck(intent_percent$percent_condition, 1),0)`, `r pluck(intent_percent$n, 1)` out of `r pluck(intent_percent$n, 1) + pluck(intent_percent$n, 2)`)), χ2(`r results$parameter`, N = `r length(intent_recode$id)`) = `r round(results$statistic,2)`, p = `r round(results$p.value, 3)`. This will knit as: Those in the reminder-through-association condition performed the intended behavior at a significantly higher rate (74%, 29 out of 39) than did those in the control condition (42, 16 out of 38)), χ2(1, N = 77) = 8.24, p = 0.004. If you’re feeling comfortable with R at this point, push yourself to reverse-engineer what each bit of this inline code is doing so that you could use it yourself (remember the ?help function). 4.2.15 Activity solutions 4.2.15.1 Activity 1 Activity 1 library(tidyverse) intent_data &lt;- read_csv(&quot;RTA_study1.csv&quot;) click the tab to see the solution 4.2.15.2 Activity 3 Activity 3 # solution using pipes intent_recode &lt;- intent_data %&gt;% filter(intend == 1) %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;rta&quot;, &quot;2&quot; = &quot;control&quot;), actualdonate = recode(actualdonate, &quot;1&quot; = &quot;donated&quot;, &quot;0&quot; = &quot;no_donation&quot;)) # solution using separate steps intent_filter &lt;- filter(intent_data, intend == 1) intent_recode &lt;- mutate(intent_filter, condition = recode(condition, &quot;1&quot; = &quot;rta&quot;, &quot;2&quot; = &quot;control&quot;), actualdonate = recode(actualdonate, &quot;1&quot; = &quot;donated&quot;, &quot;0&quot; = &quot;no_donation&quot;)) click the tab to see the solution 4.2.15.3 Activity 9 Activity 9 results$expected click the tab to see the solution 4.2.16 Test yourself ** This question is currently borked You have a dataset where gender has been coded numerically. You want to recode this to use text labels. Which code will work? mutate(gender = recode(gender, “male” = “1”, “female” = “2”, “nonbinary” = “3”)) mutate(gender = recode(gender, male = 1, “female = 2, nonbinary = 3)) mutate(gender = recode(gender,”1\" = “male”, “2” = “female”, “3” = “nonbinary”)) Explain This Answer The first option has the new and old codes in the wrong position, the second option is missing quotation marks, the third option is correct. From the below code, what would the plot have on the x-axis? exp_data gender score ggplot(data = exp_data, aes(gender, score)) From the below code, how would the bars in the plot be positioned? On top of each other Next to each other ggplot(data, aes(x = condition, fill = actualdonate)) + geom_bar() Explain This Answer Which of the following is not an argument of chisq.test() (you may need to look at the help documention to answer this question)? x y p continuity "]
]
